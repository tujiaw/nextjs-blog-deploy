---
title: '大型语言模型不知道自己不知道——这是个问题'
date: '2025-03-14'
tags: ['LLM']
draft: false
summary: 本文探讨了大型语言模型(LLMs)的一个关键缺陷：缺乏对自身能力的认知。这使得它们在执行自己并不理解的任务时过于自信，造成比幻觉问题更严重的实用性困境。
---

*大型语言模型不仅受限于幻觉问题——它们从根本上缺乏对自身能力的认知，这使得它们在执行自己并不完全理解的任务时过于自信。虽然"氛围编程"拥抱了AI快速生成解决方案的能力，但真正的进步在于能够承认模糊性、寻求澄清并认识到自身局限的模型。*

模型开发的步伐已经有所放缓，一些人对最新的GPT4.5表示失望。然而，过度关注AGI竞赛和基准性能让我们看错了方向。大型语言模型（LLMs）最广为人知和理解的弱点是它们产生"幻觉"的倾向，即模型输出在事实上不正确，却高度可信。

这当然是一个限制因素，但还有其他问题。我认为一个报道较少但可能对LLMs的实用性产生更不利影响的问题是它们缺乏对自身能力的认知。它们没有自我意识，不，我不是说它们缺乏知觉。

它们只是不知道自己擅长什么，不擅长什么。它们不知道什么是可行的，什么是不可行的。

## 氛围编程

就在几周前，Andrej Karpathy（OpenAI的创始成员之一）创造了一个术语"氛围编程"（vibe coding），这个词已经在科技界掀起了风暴，引发了诸如几天前《纽约时报》这样的大众媒体的文章。

氛围编程的要点很简单，让你的AI工具来担心代码，你只需指示（提示）AI执行你的命令。引用Andrej的话："我只是看东西，说东西，运行东西，复制粘贴东西，它基本上都能工作。"氛围感。

你可能认为幻觉会对氛围编程（氛围程序员？）产生重大影响，然而，Simon Willison认为幻觉的影响相当微小，因为当LLM"想象"出一个库或函数时，编译器或运行时会发现问题。

那这是否意味着我们可以安全地沉浸在"氛围"中？

不，远非如此。LLMs有一个微妙但更危险的弱点——它们缺乏自我认知。

## 模糊的需求

让我们以bolt（顺便说一下，我认为这是一个令人惊叹的工具）为例进行说明。如果我只是要求它"为我构建一个单词猜谜游戏"，它会非常迅速地构建"某些东西"。它创建了一个用户界面和一个半成品的游戏实现：

![bolt单词游戏]

虽然这一开始看起来相当令人印象深刻，但如果你将其与人类对同样请求的响应进行比较，就会发现一些显著的差异。一个工程师，无论多么初级，都会理解一个七字的请求充满了模糊性。基于这种"需求"就开始构建东西将是徒劳的。

人类会寻求澄清，他们会建立一个心理模型，了解充分描述任务所需的信息。他们也有足够的经验理解他们不需要知道所有细节或有详细的需求，任务（无论是编程还是其他）可以是迭代的和协作的。

Bolt对此一无所知，骄傲地将其单词游戏呈现出来，仿佛这就是成品。

## 对自身局限性的理解不足

我使用过的所有基于LLM的工具似乎都缺乏对自身局限性的理解。如果你要求它们承担远超其能力的任务，它们会勇敢地尝试，然后彻底失败！

再次以Bolt为例，我要求它构建一个相当复杂的外汇交易应用程序：

![bolt交易平台]

它做出了勇敢的尝试，渲染了一些瓷砖和图表（顺便说一下，这些并不起作用）。然而，它只不过是一个模型，肯定不是交易应用程序。

在氛围编程时，LLM忽略了模糊性并尽力而为。结果，你可以非常迅速地构建出完全错误的东西。正如其他人所述，当使用AI加速开发时，你必须投入更多时间进行测试。

但回到理解局限性的话题，这不仅仅是Bolt的问题。这是我昨晚与ChatGPT的对话，我拍了一些乐高的照片并要求提供一些构建创意：

![chatgpt乐高创意]

它开始得不错，当我要求详细说明宇宙飞船的想法，提供构建说明时，它再次开始得很强——渲染了一个令人印象深刻的乐高模型：

![chatgpt宇宙飞船]

但当涉及到说明本身时……天啊……

```
乐高星际战斗机分步说明

第1步：构建主体

基础结构：
 - 铺设一个4x8的板作为中央机身。
 - 在上面堆叠1x2和2x2砖块以增加高度。
 - 在前部安装一个斜砖以获得空气动力学形状。

驾驶舱部分：
 - 使用1x4透明或黑色部件作为挡风玻璃。
 - 添加角度斜坡来构建驾驶舱框架。

第2步：添加机翼

机翼框架：
 - 在机身两侧连接4x6或6x6板。
 - 用角度砖块加固它们以增强结构。

武器附件：
 - 在机翼边缘放置1x1夹子以固定小棒或天线。
 - 安装圆柱形乐高元素作为炮管。

[...说明继续...]
```

与之前展示的精美渲染且复杂的乐高模型相比，这些说明简直可笑地简单化。

为大型乐高模型创建详细的构建说明是一项复杂的任务。这与创建外汇交易平台的任务没有太大区别。

但再一次，LLM不了解这项任务的复杂性，也不了解自身的局限性。它只是尝试一下，然后惨败！

## 结论

随着我将LLMs用于广泛的任务，我越来越遇到这个根本性问题。幻觉当然也是一个问题，但相比之下似乎微不足道。我可以接受偶尔的事实错误，以及审查输出的需要。我发现更难与一个系统合作，该系统会愉快地承担任何任务，无论是简单但模糊的，还是复杂且远超其能力的。

目前，成功之路涉及反复试验，建立我们自己对LLM（或基于LLM的工具）能力的心理模型。这很耗时，而且永远不会非常精确，因为它们的能力当然缺乏明确的边界。

我对LLMs及其实用性仍然非常乐观，但我们不需要在一些抽象基准上表现更好的模型，我们需要能够回应"不，恐怕我不能做到"，或"我们能否分解这个任务"，或"我们能否进一步讨论这个问题"的模型和工具。

那才是真正的进步。

Scott Logic

[原文链接：https://blog.scottlogic.com/2025/03/06/llms-dont-know-what-they-dont-know-and-thats-a-problem.html](https://blog.scottlogic.com/2025/03/06/llms-dont-know-what-they-dont-know-and-thats-a-problem.html)
